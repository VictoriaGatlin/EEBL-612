{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef22358-b7c6-47ce-bfa1-4c25f82c49a7",
   "metadata": {},
   "source": [
    "# 3. `numpy` and `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e3391-e108-4225-a3d6-464544a35eff",
   "metadata": {},
   "source": [
    "This notebook follows Chapter 10 in the [Python Workshop textbook](https://search.ebscohost.com/login.aspx?direct=true&db=edsool&AN=edsool.9781804610619&site=eds-live&scope=site&authtype=shib&custid=s8516548). An electronic version of this book is freely available from the library after logging in with TAMU credentials!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f726d4",
   "metadata": {},
   "source": [
    "Taken together, pandas and NumPy are masterful at handling big data. They are built for speed, efficiency, readability, and ease of use.\n",
    "\n",
    "**Pandas** provide you with a unique framework to view and modify data. Pandas handles all data-related tasks such as creating DataFrames, importing data, scraping data from the web, merging data, pivoting, concatenating, and more.\n",
    "\n",
    "**NumPy**, short for Numerical Python, is more focused on computation. NumPy interprets the rows and columns of pandas DataFrames as matrices in the form of NumPy arrays. When computing descriptive statistics such as the mean, median, mode, and quartiles, NumPy is blazingly fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600fa2a",
   "metadata": {},
   "source": [
    "## 3.1 NumPy and basic stats\n",
    "\n",
    "NumPy is designed to handle big data swiftly. It includes the following essential components according to the NumPy documentation: \n",
    "\n",
    "- A powerful n-dimensional array object \n",
    "- Sophisticated (broadcasting) functions \n",
    "- Tools for integrating C/C++ and Fortran code \n",
    "- Useful linear algebra, Fourier transform, and random number capabilities \n",
    "\n",
    "From NumPy documentation: The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation.\n",
    "\n",
    "Going forward, instead of using lists, you will use NumPy arrays. NumPy arrays are the basic elements of the NumPy package. NumPy arrays are designed to handle arrays of any dimension. \n",
    "\n",
    "Numpy arrays can be indexed easily and can have many types of data, such as float, int, string, and object, but the **types must be consistent** to improve speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beac71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f74ded",
   "metadata": {},
   "source": [
    "### 3.1.1 Exercise 128: Converting Lists to NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = [70,65,95,88]\n",
    "type(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5c588",
   "metadata": {},
   "source": [
    "**Note** Now that numpy has been imported, you can access all numpy methods, such as numpy arrays. Type `np.` + Tab on your keyboard to see the breadth of options. You are looking for an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f34192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32dc688f",
   "metadata": {},
   "source": [
    "### 3.1.2 Exercises 129-131: Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7407199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097835d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236951ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efea010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ce697af",
   "metadata": {},
   "source": [
    "**Note** Median here is not a method of `np.array`, but it is a method of `numpy`. (The mean may be computed in the same way, as a method of numpy.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86558dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd40f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c2a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9991d02",
   "metadata": {},
   "source": [
    "## 3.2 Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451d96f",
   "metadata": {},
   "source": [
    "A DataFrame is generally composed of rows, and each row has the same number of columns. From one point of view, it's a two-dimensional grid containing lots of numbers. It can also be interpreted as a list of lists, or an array of arrays.\n",
    "\n",
    "NumPy has methods for creating matrices or n-dimensional arrays. One option is to place random numbers between 0 and 1 into each entry, as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07325b8",
   "metadata": {},
   "source": [
    "### 3.2.1 Exercise 132: Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de78ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e310981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "935fb9ec",
   "metadata": {},
   "source": [
    "**Note** The `np.random.seed()` ensures that the same collection of random numbers are drawn every time, which is important for reproducibility.  You can set your own seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80734725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fea606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f73336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing, slicing, and accessing\n",
    "\n",
    "#First row\n",
    "\n",
    "#First column\n",
    "\n",
    "#First entry\n",
    "\n",
    "#Third row, fourth column\n",
    "\n",
    "#Multiple rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix mean\n",
    "\n",
    "#First row mean\n",
    "\n",
    "#Last column mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501d0bb",
   "metadata": {},
   "source": [
    "### 3.2.1 Computation time for large matrices\n",
    "\n",
    "Now that you have gotten a hang of creating random matrices, you can see how long it takes to generate a large matrix and compute the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "np.random.seed(seed=60) \n",
    "big_matrix = np.random.rand(100000, 100)\n",
    "big_matrix.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d33715",
   "metadata": {},
   "source": [
    "In the next exercise, you will create arrays using NumPy and compute various values through them. One such computation you will be using is `ndarray.numpy.ndarray` is a (usually fixed-size) multidimensional array container of items of the same type and size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d00b0",
   "metadata": {},
   "source": [
    "### 3.2.3 Exercise 133: Creating an array to implement NumPy computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange returns evenly spaced values \n",
    "# within a given interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape to 20 rows and 5 cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3004a",
   "metadata": {},
   "source": [
    "**Note** `.reshape` fills the new matrix by rows (not by columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5200d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix computations\n",
    "\n",
    "#dot product of two arrays (for 2-d arrays, is same as matrix multiplication)\n",
    "#matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b50fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimension-specific computations\n",
    "\n",
    "\n",
    "#column means\n",
    "#row means\n",
    "\n",
    "#column std\n",
    "#row std\n",
    "\n",
    "#multi-dimensional\n",
    "mat4=np.arange(1, 13).reshape(2,2,3)\n",
    "mat4\n",
    "\n",
    "np.mean(mat4,axis=0) \n",
    "np.mean(mat4,axis=1)\n",
    "np.mean(mat4,axis=2) \n",
    "np.mean(mat4,axis=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ddc3d",
   "metadata": {},
   "source": [
    "## 3.3 The pandas library\n",
    "\n",
    "Pandas is the Python library that handles data on all fronts. Pandas can import data, read data, and display data in an object called a `DataFrame`. A DataFrame consists of rows and columns. One way to get a feel for DataFrames is to create one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d653e9",
   "metadata": {},
   "source": [
    "### 3.3.1 Exercise 134: Using DataFrames to Manipulate Stored Student testscore Data\n",
    "\n",
    "In this exercise, you will create a dictionary, which is one of many ways to create a pandas DataFrame. You will then manipulate this data as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of test scores\n",
    "test_dict = {'Corey':[63,75,88], \n",
    "             'Kevin':[48,98,92], \n",
    "             'Akshay': [87, 86, 85]}\n",
    "print(test_dict)\n",
    "\n",
    "# create dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67a365",
   "metadata": {},
   "source": [
    "You can inspect the DataFrame: \n",
    "\n",
    " - First, each dictionary key is listed as a column. \n",
    " - Second, the rows are labeled with indices starting with 0 by default. \n",
    " - Third, the visual layout is clear and legible. Each column of a DataFrame is officially represented as a Series. A series is a one-dimensional ndarray. \n",
    " \n",
    " Now, you will rotate the DataFrame, which is also known as a transpose, a standard `pandas` method. A transpose turns rows into columns and columns into rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6adb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "862ddcc7",
   "metadata": {},
   "source": [
    "### 3.3.2 Exercise 135: DataFrame Computations with the Student testscore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ccb488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8d2af83",
   "metadata": {},
   "source": [
    "Now, select a range of values from specific rows and columns. You will be using .iloc with the index number, which is a function present in a pandas DataFrame for selection. This is shown in the following step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access first row by index number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619269f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access column by name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b94c32",
   "metadata": {},
   "source": [
    "### 3.3.3 Exercise 136: Computing DataFrames within DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeaf105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a new DataFrame from first 2 rows and last 2 columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415afcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first 2 rows and last 2 columns using index numbers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1293880",
   "metadata": {},
   "source": [
    "Now, add a new column to find the quiz average of our students. You can generate new columns in a variety of ways. One way is to use available methods such as the mean. In pandas, it's important to specify the axis. An axis of 0 represents the columns, and an axis of 1 represents the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new column as mean of other columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column as a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd040f",
   "metadata": {},
   "source": [
    "In the next section, you will be looking at new rows and NaN, which is an official NumPy term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dcca77",
   "metadata": {},
   "source": [
    "### 3.3.4 New rows and NaN\n",
    "\n",
    "It's not easy to add new rows to a pandas DataFrame. A common strategy is to generate a new DataFrame and then to concatenate the values. Say you have a new student who joins the class for the fourth quiz. What values should you put for the other three quizzes? The answer is nan. It stands for not a number. nan is an official NumPy term. It can be accessed using np.nan. It is case-sensitive. In later exercises, you will look at how nan can be used. In the next exercise, you will look at concatenating and working with null values.\n",
    "\n",
    "Much more can be said on this.  **For more details**, see \n",
    " - https://pandas.pydata.org/pandas-docs/dev/user_guide/gotchas.html#nan-integer-na-values-and-na-type-promotions\n",
    " - https://pandas.pydata.org/docs/user_guide/missing_data.html\n",
    " - https://stackoverflow.com/questions/60115806/pd-na-vs-np-nan-for-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a8cef",
   "metadata": {},
   "source": [
    "### 3.3.5 Exercise 137: Concatenating and Finding the Mean with Null Values for Our testscore Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame of one row \n",
    "\n",
    "# Concatenate DataFrames \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b82922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f34c18f",
   "metadata": {},
   "source": [
    "Notice that all values are floats except for **Quiz_4**. There will be occasions when you need to cast all values in a particular column as another type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78796176",
   "metadata": {},
   "source": [
    "### 3.3.6 Casting column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f39ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "682dbebf",
   "metadata": {},
   "source": [
    "## 3.4 Data\n",
    "\n",
    "Now that you have been introduced to NumPy and pandas, you will use them to analyze some real data. Data scientists analyze data that exists in the cloud or online. One strategy is to download data directly to your computer.\n",
    " \n",
    " \n",
    "It is recommended to create a new folder to store all of your data. You can open your Jupyter Notebook in this same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e2d3d",
   "metadata": {},
   "source": [
    "### 3.4.1 Downloading data\n",
    "\n",
    "Data comes in many formats, and pandas is equipped to handle most of them. In general, when looking for data to analyze, it's worth searching the keyword \"dataset.\" A dataset is a collection of data. Online, \"data\" is everywhere, whereas datasets contain data in its raw format. You will start by examining the famous Boston Housing dataset from 1980, which is available on our GitHub repository. This dataset can be found here https://packt.live/31Cd96j. You can begin by first downloading the dataset onto our system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b44ecf",
   "metadata": {},
   "source": [
    "### 3.4.2 Reading data\n",
    "\n",
    "Here is a list of standard data files that pandas will read, along with the code for reading data:\n",
    "\n",
    "- csv files: `pd.read_csv('file_name')`\n",
    "- excel files: `pd.read_excel('file_name')`\n",
    "- feather files: `pd.read_feather('file_name')`\n",
    "- html files: `pd.read_html('file_name')`\n",
    "- json files: `pd.read_json('file_name')`\n",
    "- sql database: `pd.read_sql('file_name')`\n",
    "\n",
    "If the files are clean, pandas will read them properly. Sometimes, files are not clean, and changing function parameters may be required. It's advisable to copy any errors and search for solutions online. A further point of consideration is that the data should be read into a DataFrame. Pandas will convert the data into a DataFrame upon reading it, but you need to save DataFrame as a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557821f3",
   "metadata": {},
   "source": [
    "### 3.4.3 Exercise 138: Reading and viewing the Boston Housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv('HousingData.csv')\n",
    "housing_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6ceaa37",
   "metadata": {},
   "source": [
    "Data description can be found at this link: https://search.r-project.org/CRAN/refmans/mlbench/html/BostonHousing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb814bdc",
   "metadata": {},
   "source": [
    "### 3.4.4 Exercise 139: Gaining data insights from the Boston Housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018bd0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469f4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00987d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c4e72ee",
   "metadata": {},
   "source": [
    "This confirms that you have 506 rows and 14 columns. Notice that shape does not have any parentheses after it. This is because it's technically an attribute and pre-computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204df4c1",
   "metadata": {},
   "source": [
    "### 3.4.5 Null values\n",
    "\n",
    "You need to do something about the null values. There are several popular choices when dealing with null values: \n",
    "\n",
    "- Eliminate the rows: Can work if null values are a very small percentage, such as 1% of the total dataset. \n",
    "- Replace missing values with the mean/median/mode and add a missing indicator (for use in downstream modeling efforts)\n",
    "- Impute missing values: Depends on the reason for missingness.  Can use other fields to impute missing values in a given field if it is reasonable to assume that missingness can be \"explained\" by other **observed** values.  This is not always the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ef6aa",
   "metadata": {},
   "source": [
    "### 3.4.6 Exercise 140: Null value operations on a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99314d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b623f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61b084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5565ea",
   "metadata": {},
   "source": [
    "Breakdown of the above code:\n",
    "- `housing_df` is the DataFrame. \n",
    "- `.loc` allows you to specify rows and columns. \n",
    "- `:` selects all rows. \n",
    "- `housing_df.isnull().any()` selects only columns with null values.\n",
    "- `.describe()` pulls up the statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833276f7",
   "metadata": {},
   "source": [
    "### 3.4.7 Replacing null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0442c",
   "metadata": {},
   "source": [
    "Pandas include a nice method, `fillna`, which can be used to replace null values. It works for individual columns and entire DataFrames. You will use three approaches, \n",
    "\n",
    "- replacing the null values of a column with the mean\n",
    "- replacing the null values of a column with another value\n",
    "- replacing all the null values in the entire dataset with the median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14babd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing with the mean\n",
    "\n",
    "# replacing with another value\n",
    "\n",
    "# replacing with median\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f711b",
   "metadata": {},
   "source": [
    "After eliminating all null values, the dataset is much cleaner. There may also be unrealistic outliers or extreme outliers that will lead to poor prediction. These can often be detected through visual analysis, which you will be covering in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891104a3",
   "metadata": {},
   "source": [
    "## 3.5 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e78b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up seaborn dark grid\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f6d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24b8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616e663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76feb2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c88c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274f934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d195b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77485b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a44c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2c18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bebf2f76",
   "metadata": {},
   "source": [
    "You began our introduction to data analysis with **NumPy**, Python's incredibly fast library for handling massive matrix computations. Next, you learned about the fundamentals of **pandas**, Python's library for handling DataFrames. Taken together, you used NumPy and pandas to analyze the Boston Housing dataset, which included descriptive statistical methods and Matplotlib and Seaborn's graphical libraries. You also learned about advanced methods for creating clean, clearly labeled, publishable graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14901f45",
   "metadata": {},
   "source": [
    "## 3.6 Fun example: Broadcasting is not always faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27595b",
   "metadata": {},
   "source": [
    "From https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "\n",
    "\"Broadcasting is a powerful tool for writing short and usually intuitive code that does its computations very efficiently in C. However, there are cases when broadcasting uses unnecessarily large amounts of memory for a particular algorithm. In these cases, it is better to write the algorithm’s outer loop in Python. This may also produce more readable code, as algorithms that use broadcasting tend to become more difficult to interpret as the number of dimensions in the broadcast increases.\"\n",
    "\n",
    "See the following example and explanation from https://stackoverflow.com/questions/49632993/why-python-broadcasting-in-the-example-below-is-slower-than-a-simple-loop.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f808d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to take squared some of rows after row-wise subtraction\n",
    "\n",
    "def norm_loop(M, v):\n",
    "  n = M.shape[0]\n",
    "  d = np.zeros(n)\n",
    "  for i in range(n):\n",
    "    d[i] = np.sum((M[i] - v)**2)\n",
    "  return d\n",
    "\n",
    "def norm_bcast(M, v):\n",
    "     return np.sum((M - v)**2, axis=1)\n",
    "\n",
    "#broadcasting is better in this instance for smaller datasets\n",
    "M = np.random.random_sample((10, 100))\n",
    "v = M[0]\n",
    "%timeit norm_loop(M, v) \n",
    "%timeit norm_bcast(M, v)\n",
    "\n",
    "#bigger datasets tell a different story\n",
    "M = np.random.random_sample((1000, 10000))\n",
    "v = M[0]\n",
    "%timeit norm_loop(M, v) \n",
    "%timeit norm_bcast(M, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b0d19",
   "metadata": {},
   "source": [
    "What gives? It comes down to **memory access**. \n",
    "\n",
    "In the broadcast version, every element of `M` is subtracted from `v`. By the time the last row of `M` is processed, the results of processing the first row have been evicted from cache, so for the second step, these differences are again loaded into cache memory and squared. Finally, they are loaded and processed a third time for the summation. Since `M` is quite large, parts of the cache are cleared on each step to accommodate all of the data.\n",
    "\n",
    "In the looped version, each row is processed completely in one smaller step, leading to fewer cache misses (i.e. inability to retreive needed data from cache because it has been cleared and needs to be reloaded) and overall faster code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
